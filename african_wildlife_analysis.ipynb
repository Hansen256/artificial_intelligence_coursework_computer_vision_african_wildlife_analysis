{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05e611d",
   "metadata": {},
   "source": [
    "# African Wildlife Image Analysis\n",
    "**Computer Vision Coursework - Group 1**\n",
    "\n",
    "This notebook covers:\n",
    "- **Part A:** OpenCV Image Processing (Filtering, Edge Detection, Contours)\n",
    "- **Part B:** Deep Learning Classification using MobileNetV2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c46136",
   "metadata": {},
   "source": [
    "## ðŸ”§ Setup Section\n",
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a98cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install kaggle API for dataset download\n",
    "!pip install -q kaggle\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"OpenCV Version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c4090",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21013d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(\"ðŸš€ GPU Available:\", gpu_devices[0].name)\n",
    "    print(\"Training will be FAST!\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected. Training will be slower.\")\n",
    "    print(\"To enable GPU: Runtime â†’ Change runtime type â†’ T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1533630",
   "metadata": {},
   "source": [
    "### Mount Google Drive (to save results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ad6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to save models and results\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create a folder for this project\n",
    "project_folder = '/content/drive/MyDrive/african_wildlife_project'\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "print(f\"âœ… Project folder created: {project_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa067eb2",
   "metadata": {},
   "source": [
    "### Upload Kaggle API Key\n",
    "**âš ï¸ ACTION REQUIRED:** Click the \"Choose Files\" button below and upload your `kaggle.json` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd1fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your kaggle.json file\n",
    "print(\"ðŸ“¤ Please upload your kaggle.json file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Set up Kaggle API credentials\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"âœ… Kaggle API configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f434af7",
   "metadata": {},
   "source": [
    "### Download African Wildlife Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f083348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from Kaggle\n",
    "print(\"â¬‡ï¸ Downloading African Wildlife dataset...\")\n",
    "!kaggle datasets download -d biancaferreira/african-wildlife\n",
    "\n",
    "# Unzip the dataset\n",
    "print(\"ðŸ“¦ Extracting dataset...\")\n",
    "!unzip -q african-wildlife.zip -d /content/african_wildlife\n",
    "\n",
    "print(\"âœ… Dataset downloaded and extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef09a7",
   "metadata": {},
   "source": [
    "### Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c578d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "dataset_path = '/content/african_wildlife'\n",
    "\n",
    "# Find all subdirectories (animal classes)\n",
    "def explore_dataset(path):\n",
    "    print(\"ðŸ“ Dataset Structure:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Look for common dataset structures\n",
    "    possible_dirs = [path, os.path.join(path, 'train'), os.path.join(path, 'dataset')]\n",
    "    \n",
    "    found_dir = None\n",
    "    for base_dir in possible_dirs:\n",
    "        if os.path.exists(base_dir):\n",
    "            items = os.listdir(base_dir)\n",
    "            print(f\"\\nContents of {base_dir}:\")\n",
    "            subdirs_with_images = []\n",
    "            \n",
    "            for item in items:\n",
    "                item_path = os.path.join(base_dir, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    # Count images in this directory\n",
    "                    image_files = [f for f in os.listdir(item_path) \n",
    "                                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                    if image_files:\n",
    "                        subdirs_with_images.append(item)\n",
    "                        print(f\"  ðŸ“‚ {item}: {len(image_files)} images\")\n",
    "            \n",
    "            # If this directory has subdirectories with images, use it\n",
    "            if subdirs_with_images and found_dir is None:\n",
    "                found_dir = base_dir\n",
    "    \n",
    "    # Return the directory that has subdirectories with images, or default to the input path\n",
    "    return found_dir if found_dir else path\n",
    "\n",
    "base_dir = explore_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb2ddc6",
   "metadata": {},
   "source": [
    "### Select Animal Classes for Analysis\n",
    "**âš ï¸ DECISION POINT:** Based on the structure above, we'll select 4 animal classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correct data path based on exploration\n",
    "# Use the base_dir from the previous cell\n",
    "data_dir = base_dir  # Use the detected directory from explore_dataset\n",
    "\n",
    "# List all available classes\n",
    "all_classes = [d for d in os.listdir(data_dir) \n",
    "               if os.path.isdir(os.path.join(data_dir, d)) and not d.startswith('.')]\n",
    "\n",
    "print(\"Available animal classes:\")\n",
    "for i, cls in enumerate(all_classes, 1):\n",
    "    print(f\"{i}. {cls}\")\n",
    "\n",
    "# Select 4 classes with good representation\n",
    "# You can manually change these based on what you see above\n",
    "if len(all_classes) >= 4:\n",
    "    selected_classes = all_classes[:4]  # Take first 4\n",
    "else:\n",
    "    selected_classes = all_classes\n",
    "\n",
    "print(f\"\\nâœ… Selected classes for analysis: {selected_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72a2f1",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“¸ Part A: OpenCV Image Processing\n",
    "### Task 1: Load Sample Images from Different Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd76bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load sample images from each class\n",
    "def load_sample_images(data_dir, classes, num_per_class=3):\n",
    "    \"\"\"\n",
    "    Load sample images from each animal class\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to dataset directory\n",
    "        classes: List of class names\n",
    "        num_per_class: Number of images to load per class\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with class names as keys and lists of images as values\n",
    "    \"\"\"\n",
    "    sample_images = {}\n",
    "    \n",
    "    for cls in classes:\n",
    "        class_path = os.path.join(data_dir, cls)\n",
    "        image_files = [f for f in os.listdir(class_path) \n",
    "                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        # Load first 'num_per_class' images\n",
    "        sample_images[cls] = []\n",
    "        for img_file in image_files[:num_per_class]:\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            # Read image using OpenCV (BGR format)\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            # Check if image loaded successfully\n",
    "            if img is None:\n",
    "                print(f\"âš ï¸ Warning: Could not load {img_path}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Convert BGR to RGB for proper display\n",
    "            try:\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                sample_images[cls].append(img_rgb)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Warning: Could not convert {img_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return sample_images\n",
    "\n",
    "# Load sample images\n",
    "sample_images = load_sample_images(data_dir, selected_classes, num_per_class=3)\n",
    "\n",
    "# Display loaded images\n",
    "print(f\"âœ… Loaded {sum(len(imgs) for imgs in sample_images.values())} images total\")\n",
    "for cls, imgs in sample_images.items():\n",
    "    print(f\"  - {cls}: {len(imgs)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ebacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "fig, axes = plt.subplots(len(selected_classes), 3, figsize=(12, 4*len(selected_classes)))\n",
    "fig.suptitle('Sample Images from Each Animal Class', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, (cls, imgs) in enumerate(sample_images.items()):\n",
    "    for j, img in enumerate(imgs):\n",
    "        # Handle both single class (1D array) and multiple classes (2D array)\n",
    "        if len(selected_classes) == 1:\n",
    "            ax = axes[j]\n",
    "        else:\n",
    "            ax = axes[i, j]\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if j == 0:\n",
    "            ax.set_title(f'{cls.upper()}', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{project_folder}/01_sample_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¾ Saved to Google Drive: 01_sample_images.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10a2cc",
   "metadata": {},
   "source": [
    "### Task 2: Convert to Grayscale and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de27d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one image from each class for detailed processing\n",
    "selected_for_processing = {cls: imgs[0] for cls, imgs in sample_images.items()}\n",
    "\n",
    "# Convert to grayscale\n",
    "grayscale_images = {}\n",
    "for cls, img in selected_for_processing.items():\n",
    "    # Convert RGB to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    grayscale_images[cls] = gray\n",
    "\n",
    "print(\"âœ… Converted images to grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8882d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original vs grayscale comparison\n",
    "fig, axes = plt.subplots(len(selected_classes), 2, figsize=(10, 5*len(selected_classes)))\n",
    "fig.suptitle('Original vs Grayscale Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, cls in enumerate(selected_classes):\n",
    "    # Handle both single class (1D array) and multiple classes (2D array)\n",
    "    if len(selected_classes) == 1:\n",
    "        ax1 = axes[0]\n",
    "        ax2 = axes[1]\n",
    "    else:\n",
    "        ax1 = axes[i, 0]\n",
    "        ax2 = axes[i, 1]\n",
    "    \n",
    "    # Original image\n",
    "    ax1.imshow(selected_for_processing[cls])\n",
    "    ax1.set_title(f'{cls} - Original (RGB)', fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Grayscale image\n",
    "    ax2.imshow(grayscale_images[cls], cmap='gray')\n",
    "    ax2.set_title(f'{cls} - Grayscale', fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{project_folder}/02_grayscale_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¾ Saved to Google Drive: 02_grayscale_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b04f483",
   "metadata": {},
   "source": [
    "### Task 3: Apply Gaussian and Median Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filters to grayscale images\n",
    "gaussian_filtered = {}\n",
    "median_filtered = {}\n",
    "\n",
    "for cls, gray_img in grayscale_images.items():\n",
    "    # Gaussian Blur: Smooths image using weighted average (good for reducing noise)\n",
    "    # Kernel size (5, 5) - must be odd numbers\n",
    "    # sigmaX=0 means calculate automatically\n",
    "    gaussian = cv2.GaussianBlur(gray_img, (5, 5), 0)\n",
    "    gaussian_filtered[cls] = gaussian\n",
    "    \n",
    "    # Median Filter: Replaces each pixel with median of neighboring pixels\n",
    "    # Kernel size 5 - good for removing salt-and-pepper noise\n",
    "    median = cv2.medianBlur(gray_img, 5)\n",
    "    median_filtered[cls] = median\n",
    "\n",
    "print(\"âœ… Applied Gaussian and Median filters\")\n",
    "print(\"\\nðŸ“ Filter Details:\")\n",
    "print(\"  - Gaussian Blur: Kernel size (5x5), reduces Gaussian noise\")\n",
    "print(\"  - Median Filter: Kernel size 5, removes salt-and-pepper noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f90a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison: Original, Gaussian, Median\n",
    "fig, axes = plt.subplots(len(selected_classes), 3, figsize=(15, 5*len(selected_classes)))\n",
    "fig.suptitle('Filter Comparison: Original vs Gaussian vs Median', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, cls in enumerate(selected_classes):\n",
    "    # Handle both single class (1D array) and multiple classes (2D array)\n",
    "    if len(selected_classes) == 1:\n",
    "        ax1 = axes[0]\n",
    "        ax2 = axes[1]\n",
    "        ax3 = axes[2]\n",
    "    else:\n",
    "        ax1 = axes[i, 0]\n",
    "        ax2 = axes[i, 1]\n",
    "        ax3 = axes[i, 2]\n",
    "    \n",
    "    # Original grayscale\n",
    "    ax1.imshow(grayscale_images[cls], cmap='gray')\n",
    "    ax1.set_title(f'{cls} - Original', fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Gaussian filtered\n",
    "    ax2.imshow(gaussian_filtered[cls], cmap='gray')\n",
    "    ax2.set_title(f'{cls} - Gaussian Blur', fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Median filtered\n",
    "    ax3.imshow(median_filtered[cls], cmap='gray')\n",
    "    ax3.set_title(f'{cls} - Median Filter', fontweight='bold')\n",
    "    ax3.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{project_folder}/03_filter_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¾ Saved to Google Drive: 03_filter_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be8b9e",
   "metadata": {},
   "source": [
    "### Task 4: Edge Detection - Canny and Sobel Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply edge detection algorithms\n",
    "canny_edges = {}\n",
    "sobel_edges = {}\n",
    "\n",
    "for cls, gray_img in grayscale_images.items():\n",
    "    # Canny Edge Detection\n",
    "    # Parameters: image, threshold1 (lower), threshold2 (upper)\n",
    "    # If gradient > threshold2 â†’ strong edge\n",
    "    # If gradient < threshold1 â†’ not an edge\n",
    "    # In between â†’ weak edge (kept if connected to strong edge)\n",
    "    canny = cv2.Canny(gray_img, threshold1=50, threshold2=150)\n",
    "    canny_edges[cls] = canny\n",
    "    \n",
    "    # Sobel Edge Detection\n",
    "    # Detects edges by calculating gradient in X and Y directions\n",
    "    # cv2.CV_64F: Output data type (64-bit float for accuracy)\n",
    "    # dx=1, dy=0: Gradient in X direction (vertical edges)\n",
    "    sobel_x = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    # dx=0, dy=1: Gradient in Y direction (horizontal edges)\n",
    "    sobel_y = cv2.Sobel(gray_img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    \n",
    "    # Combine X and Y gradients to get magnitude\n",
    "    sobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)\n",
    "    \n",
    "    # Normalize to 0-255 range for display (with safety check for uniform images)\n",
    "    max_val = np.max(sobel_combined)\n",
    "    if max_val > 0:\n",
    "        sobel_combined = np.uint8(255 * sobel_combined / max_val)\n",
    "    else:\n",
    "        sobel_combined = np.uint8(sobel_combined)\n",
    "    \n",
    "    sobel_edges[cls] = sobel_combined\n",
    "\n",
    "print(\"âœ… Applied Canny and Sobel edge detection\")\n",
    "print(\"\\nðŸ“ Edge Detection Details:\")\n",
    "print(\"  - Canny: Multi-stage algorithm, detects edges with precision\")\n",
    "print(\"  - Sobel: Gradient-based, detects edges using derivatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4af13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display edge detection comparison\n",
    "fig, axes = plt.subplots(len(selected_classes), 3, figsize=(15, 5*len(selected_classes)))\n",
    "fig.suptitle('Edge Detection: Original vs Canny vs Sobel', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, cls in enumerate(selected_classes):\n",
    "    # Handle both single class (1D array) and multiple classes (2D array)\n",
    "    if len(selected_classes) == 1:\n",
    "        ax1 = axes[0]\n",
    "        ax2 = axes[1]\n",
    "        ax3 = axes[2]\n",
    "    else:\n",
    "        ax1 = axes[i, 0]\n",
    "        ax2 = axes[i, 1]\n",
    "        ax3 = axes[i, 2]\n",
    "    \n",
    "    # Original grayscale\n",
    "    ax1.imshow(grayscale_images[cls], cmap='gray')\n",
    "    ax1.set_title(f'{cls} - Original', fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Canny edges\n",
    "    ax2.imshow(canny_edges[cls], cmap='gray')\n",
    "    ax2.set_title(f'{cls} - Canny Edges', fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Sobel edges\n",
    "    ax3.imshow(sobel_edges[cls], cmap='gray')\n",
    "    ax3.set_title(f'{cls} - Sobel Edges', fontweight='bold')\n",
    "    ax3.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{project_folder}/04_edge_detection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¾ Saved to Google Drive: 04_edge_detection.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac666c4",
   "metadata": {},
   "source": [
    "### Task 5: Detect and Highlight Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d999405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and draw contours on images\n",
    "contour_images = {}\n",
    "contour_counts = {}\n",
    "\n",
    "for cls in selected_classes:\n",
    "    # Use Canny edges for contour detection\n",
    "    edges = canny_edges[cls]\n",
    "    \n",
    "    # Find contours\n",
    "    # cv2.RETR_EXTERNAL: Retrieve only external (outermost) contours\n",
    "    # cv2.CHAIN_APPROX_SIMPLE: Compress horizontal/vertical segments\n",
    "    contours, hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Create a copy of original image to draw contours on\n",
    "    img_with_contours = selected_for_processing[cls].copy()\n",
    "    \n",
    "    # Filter contours by area to remove small noise\n",
    "    min_contour_area = 500  # Minimum area threshold\n",
    "    significant_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_contour_area]\n",
    "    \n",
    "    # Draw contours on image\n",
    "    # -1 means draw all contours\n",
    "    # (0, 255, 0) is green color in RGB\n",
    "    # 2 is line thickness\n",
    "    cv2.drawContours(img_with_contours, significant_contours, -1, (0, 255, 0), 2)\n",
    "    \n",
    "    contour_images[cls] = img_with_contours\n",
    "    contour_counts[cls] = len(significant_contours)\n",
    "\n",
    "print(\"âœ… Detected and highlighted contours\")\n",
    "print(\"\\nðŸ“Š Contour Counts:\")\n",
    "for cls, count in contour_counts.items():\n",
    "    print(f\"  - {cls}: {count} significant contours detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a604ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images with contours highlighted\n",
    "fig, axes = plt.subplots(len(selected_classes), 2, figsize=(12, 6*len(selected_classes)))\n",
    "fig.suptitle('Contour Detection on Animal Shapes', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, cls in enumerate(selected_classes):\n",
    "    # Handle both single class (1D array) and multiple classes (2D array)\n",
    "    if len(selected_classes) == 1:\n",
    "        ax1 = axes[0]\n",
    "        ax2 = axes[1]\n",
    "    else:\n",
    "        ax1 = axes[i, 0]\n",
    "        ax2 = axes[i, 1]\n",
    "    \n",
    "    # Original image\n",
    "    ax1.imshow(selected_for_processing[cls])\n",
    "    ax1.set_title(f'{cls} - Original', fontweight='bold')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Image with contours\n",
    "    ax2.imshow(contour_images[cls])\n",
    "    ax2.set_title(f'{cls} - Contours ({contour_counts[cls]} detected)', fontweight='bold')\n",
    "    ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{project_folder}/05_contour_detection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¾ Saved to Google Drive: 05_contour_detection.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c84d5a",
   "metadata": {},
   "source": [
    "### Task 6: Analysis - Image Clarity and Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676b6b7",
   "metadata": {},
   "source": [
    "#### ðŸ“ Observations on Image Processing:\n",
    "\n",
    "**1. Grayscale Conversion:**\n",
    "- Reduces image complexity from 3 channels (RGB) to 1 channel (intensity)\n",
    "- Makes edge detection more efficient and accurate\n",
    "- Preserves structural information while removing color information\n",
    "\n",
    "**2. Filtering Effects:**\n",
    "- **Gaussian Blur:** Smooths images, reduces high-frequency noise, creates softer edges\n",
    "- **Median Filter:** Better at preserving edges while removing noise, especially salt-and-pepper noise\n",
    "- Both filters help improve edge detection by reducing false edges from noise\n",
    "\n",
    "**3. Edge Detection Comparison:**\n",
    "- **Canny:** More precise, detects thin edges, good for clean boundaries\n",
    "- **Sobel:** Captures gradient magnitude, shows edge strength, more tolerant to noise\n",
    "\n",
    "**4. Contours and Animal Shapes:**\n",
    "- Contours successfully outline the main animal bodies\n",
    "- Clear boundaries (like elephant ears, zebra body) produce strong contours\n",
    "- Complex backgrounds may create additional contours\n",
    "- Filtering before edge detection improves contour quality\n",
    "\n",
    "**5. Relationship Between Image Clarity and Edges:**\n",
    "- Sharper images â†’ clearer edges â†’ better contour detection\n",
    "- High contrast between animal and background â†’ stronger edge signals\n",
    "- Animal features (stripes, ears, tusks) create distinctive edge patterns\n",
    "- Blurred or low-contrast areas result in weak or missing edges\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9898ac",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ¤– Part B: Deep Learning Classification\n",
    "### Task 7: Prepare Dataset - Resize and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset dataset with selected classes\n",
    "subset_dir = '/content/dataset_subset'\n",
    "os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "MAX_IMAGES_PER_CLASS = 150  # Limit images per class for faster training\n",
    "IMG_SIZE = (224, 224)  # Standard size for MobileNetV2\n",
    "\n",
    "print(\"ðŸ“¦ Creating dataset subset...\")\n",
    "for cls in selected_classes:\n",
    "    src_dir = os.path.join(data_dir, cls)\n",
    "    dst_dir = os.path.join(subset_dir, cls)\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(src_dir) \n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    # Copy limited number of images\n",
    "    for img_file in image_files[:MAX_IMAGES_PER_CLASS]:\n",
    "        src_path = os.path.join(src_dir, img_file)\n",
    "        dst_path = os.path.join(dst_dir, img_file)\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "    \n",
    "    print(f\"  âœ… {cls}: {len(os.listdir(dst_dir))} images\")\n",
    "\n",
    "print(f\"\\nâœ… Dataset subset created at: {subset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40081257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation splits using Keras utilities\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "VALIDATION_SPLIT = 0.2  # 80% train, 20% validation\n",
    "\n",
    "# Training dataset\n",
    "# image_dataset_from_directory automatically:\n",
    "# - Loads images from subdirectories (each subdirectory = one class)\n",
    "# - Resizes images to specified size\n",
    "# - Creates batches\n",
    "# - Shuffles data\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    subset_dir,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"training\",\n",
    "    seed=123,  # For reproducibility\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    subset_dir,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Get class names\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"âœ… Dataset prepared:\")\n",
    "print(f\"  - Classes: {class_names}\")\n",
    "print(f\"  - Number of classes: {num_classes}\")\n",
    "print(f\"  - Training batches: {len(train_ds)}\")\n",
    "print(f\"  - Validation batches: {len(val_ds)}\")\n",
    "print(f\"  - Image size: {IMG_HEIGHT}x{IMG_WIDTH}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35fc8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images to [0, 1] range\n",
    "# MobileNetV2 expects pixel values in [0, 1]\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "# Apply normalization to datasets\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Optimize performance with caching and prefetching\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"âœ… Images normalized to [0, 1] range\")\n",
    "print(\"âœ… Dataset optimized with caching and prefetching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164ec0c",
   "metadata": {},
   "source": [
    "### Task 8-9: Load Pretrained MobileNetV2 and Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e8ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained MobileNetV2 model\n",
    "# weights='imagenet': Use weights pretrained on ImageNet dataset\n",
    "# include_top=False: Exclude the final classification layer (we'll add our own)\n",
    "# input_shape: Size of input images\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze the base model layers\n",
    "# This prevents the pretrained weights from being updated during training\n",
    "# We only train the new layers we add on top\n",
    "base_model.trainable = False\n",
    "\n",
    "print(\"âœ… Loaded pretrained MobileNetV2\")\n",
    "print(f\"  - Total layers in base model: {len(base_model.layers)}\")\n",
    "print(f\"  - Base model trainable: {base_model.trainable}\")\n",
    "print(f\"  - Input shape: {IMG_HEIGHT}x{IMG_WIDTH}x3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the complete model\n",
    "# Add custom layers on top of MobileNetV2\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,  # Pretrained MobileNetV2\n",
    "    \n",
    "    # Global Average Pooling: Converts 2D feature maps to 1D feature vector\n",
    "    # Reduces spatial dimensions while preserving important features\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    \n",
    "    # Dropout: Randomly drops 20% of neurons during training\n",
    "    # Helps prevent overfitting\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    # Dense layer: Final classification layer\n",
    "    # num_classes outputs (one for each animal class)\n",
    "    # softmax activation: Converts outputs to probabilities that sum to 1\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"âœ… Model architecture created\")\n",
    "print(\"\\nðŸ“Š Model Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# Optimizer: Adam - adaptive learning rate optimizer\n",
    "# Loss: Sparse Categorical Crossentropy - for multi-class classification\n",
    "# Metrics: Accuracy - track classification accuracy\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ… Model compiled and ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0194ba",
   "metadata": {},
   "source": [
    "### Task 10: Train Model (5-10 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a95ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 10  # Number of times to iterate over the entire dataset\n",
    "\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "print(f\"  - Epochs: {EPOCHS}\")\n",
    "print(f\"  - Training samples per epoch: ~{len(train_ds) * BATCH_SIZE}\")\n",
    "print(f\"  - Validation samples per epoch: ~{len(val_ds) * BATCH_SIZE}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1  # Show progress bar\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee865892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "ax1.set_title('Model Accuracy over Epochs', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "ax2.set_title('Model Loss over Epochs', fontweight='bold', fontsize=14)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{project_folder}/06_training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "print(\"\\nðŸ“Š Final Training Metrics:\")\n",
    "print(f\"  - Training Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
    "print(f\"  - Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
    "print(f\"  - Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"  - Validation Loss: {final_val_loss:.4f}\")\n",
    "print(\"\\nðŸ’¾ Saved to Google Drive: 06_training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602cfe3d",
   "metadata": {},
   "source": [
    "### Task 11: Confusion Matrix and Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b01fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation set\n",
    "print(\"ðŸ”® Generating predictions on validation set...\")\n",
    "\n",
    "# Get all validation data\n",
    "y_true = []\n",
    "y_pred = []\n",
    "val_images = []\n",
    "\n",
    "for images, labels in val_ds:\n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "    val_images.extend(images.numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "val_images = np.array(val_images)\n",
    "\n",
    "print(f\"âœ… Generated predictions for {len(y_true)} validation images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2504e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Animal Classification', fontweight='bold', fontsize=16)\n",
    "plt.ylabel('True Label', fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{project_folder}/07_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¾ Saved to Google Drive: 07_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nðŸ“ˆ Per-Class Accuracy:\")\n",
    "print(\"=\"*60)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = y_true == i\n",
    "    num_samples = np.sum(class_mask)\n",
    "    \n",
    "    # Check if there are samples for this class\n",
    "    if num_samples > 0:\n",
    "        class_accuracy = np.sum(y_pred[class_mask] == y_true[class_mask]) / num_samples\n",
    "        print(f\"  {class_name}: {class_accuracy:.4f} ({class_accuracy*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  {class_name}: No samples in validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c6f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample predictions\n",
    "# Select 12 random samples\n",
    "num_samples = min(12, len(val_images))\n",
    "random_indices = np.random.choice(len(val_images), num_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle('Sample Predictions on Validation Set', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < num_samples:\n",
    "        img_idx = random_indices[idx]\n",
    "        img = val_images[img_idx]\n",
    "        true_label = class_names[y_true[img_idx]]\n",
    "        pred_label = class_names[y_pred[img_idx]]\n",
    "        \n",
    "        # Determine if prediction is correct\n",
    "        is_correct = y_true[img_idx] == y_pred[img_idx]\n",
    "        color = 'green' if is_correct else 'red'\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'True: {true_label}\\nPred: {pred_label}', \n",
    "                     color=color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{project_folder}/08_sample_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¾ Saved to Google Drive: 08_sample_predictions.png\")\n",
    "print(\"\\nðŸŸ¢ Green = Correct Prediction\")\n",
    "print(\"ðŸ”´ Red = Incorrect Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a764cee",
   "metadata": {},
   "source": [
    "### Task 12: Discussion - Accuracy Differences and Causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd939d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications\n",
    "misclassified_mask = y_true != y_pred\n",
    "num_misclassified = np.sum(misclassified_mask)\n",
    "total_samples = len(y_true)\n",
    "\n",
    "print(\"ðŸ” Misclassification Analysis:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total validation samples: {total_samples}\")\n",
    "print(f\"Correctly classified: {total_samples - num_misclassified}\")\n",
    "print(f\"Misclassified: {num_misclassified}\")\n",
    "print(f\"Overall accuracy: {(total_samples - num_misclassified) / total_samples * 100:.2f}%\")\n",
    "\n",
    "# Find most common misclassifications\n",
    "print(\"\\nâŒ Most Common Misclassifications:\")\n",
    "print(\"=\"*60)\n",
    "misclass_pairs = {}\n",
    "for true_idx, pred_idx in zip(y_true[misclassified_mask], y_pred[misclassified_mask]):\n",
    "    pair = (class_names[true_idx], class_names[pred_idx])\n",
    "    misclass_pairs[pair] = misclass_pairs.get(pair, 0) + 1\n",
    "\n",
    "# Sort by frequency\n",
    "sorted_misclass = sorted(misclass_pairs.items(), key=lambda x: x[1], reverse=True)\n",
    "for (true_class, pred_class), count in sorted_misclass[:5]:\n",
    "    print(f\"  {true_class} â†’ {pred_class}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c5a7ad",
   "metadata": {},
   "source": [
    "#### ðŸ“ Discussion: Accuracy Differences Across Classes\n",
    "\n",
    "**Possible Causes of Accuracy Variations:**\n",
    "\n",
    "**1. Visual Similarity Between Classes:**\n",
    "- Some animals may share similar features (body shape, color, texture)\n",
    "- Example: If buffalo and rhino have similar body structures, the model might confuse them\n",
    "- Animals with distinctive features (zebra stripes, elephant trunk) are easier to classify\n",
    "\n",
    "**2. Dataset Imbalance:**\n",
    "- Classes with more training images generally achieve higher accuracy\n",
    "- Uneven distribution of samples can bias the model toward majority classes\n",
    "- Limited samples for some classes lead to underfitting\n",
    "\n",
    "**3. Image Quality and Variety:**\n",
    "- **Lighting conditions:** Dark or overexposed images are harder to classify\n",
    "- **Viewing angles:** Side views vs. front views affect feature visibility\n",
    "- **Occlusions:** Animals partially hidden by vegetation or other objects\n",
    "- **Image resolution:** Blurry or low-resolution images lack discriminative features\n",
    "\n",
    "**4. Background Complexity:**\n",
    "- Cluttered backgrounds can confuse the model\n",
    "- Animals in their natural habitat may blend with surroundings\n",
    "- Simple backgrounds make animal features more prominent\n",
    "\n",
    "**5. Pose and Position Variations:**\n",
    "- Standing, sitting, or lying positions change appearance significantly\n",
    "- Multiple animals in one image can confuse the classifier\n",
    "- Unusual poses not well-represented in training data lead to errors\n",
    "\n",
    "**6. Model Limitations:**\n",
    "- **Transfer learning constraints:** MobileNetV2 was pretrained on ImageNet, which may not perfectly align with wildlife images\n",
    "- **Limited fine-tuning:** Only training top layers (base model frozen) restricts adaptation\n",
    "- **Model capacity:** MobileNetV2 is lightweight - more complex models might achieve better accuracy\n",
    "\n",
    "**7. Training Parameters:**\n",
    "- **Epochs:** 10 epochs might be insufficient for full convergence\n",
    "- **Learning rate:** Default Adam settings may not be optimal\n",
    "- **Batch size:** Affects gradient updates and generalization\n",
    "\n",
    "**Improvements to Consider:**\n",
    "- Increase training data through data augmentation (rotation, flip, zoom)\n",
    "- Balance dataset across all classes\n",
    "- Unfreeze and fine-tune deeper layers of base model\n",
    "- Train for more epochs with early stopping\n",
    "- Experiment with different pretrained models (ResNet50, EfficientNet)\n",
    "- Apply better preprocessing (background removal, cropping)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b02aa",
   "metadata": {},
   "source": [
    "### Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to Google Drive\n",
    "model_save_path = f'{project_folder}/african_wildlife_model.keras'\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(f\"âœ… Model saved to: {model_save_path}\")\n",
    "print(\"\\nðŸ“¦ You can download this model from your Google Drive later!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b386f",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ‰ Assignment Complete!\n",
    "\n",
    "### Summary of Results:\n",
    "\n",
    "**Part A - OpenCV Image Processing:**\n",
    "âœ… Loaded and processed images from multiple animal classes\n",
    "âœ… Applied grayscale conversion and compared with originals\n",
    "âœ… Implemented Gaussian and Median filtering\n",
    "âœ… Performed edge detection using Canny and Sobel operators\n",
    "âœ… Detected and highlighted contours on animal shapes\n",
    "âœ… Analyzed relationship between image clarity and edge detection\n",
    "\n",
    "**Part B - Deep Learning Classification:**\n",
    "âœ… Prepared dataset with resizing and normalization\n",
    "âœ… Used pretrained MobileNetV2 model\n",
    "âœ… Fine-tuned on selected animal categories\n",
    "âœ… Trained for 10 epochs with validation monitoring\n",
    "âœ… Generated confusion matrix and classification report\n",
    "âœ… Displayed sample predictions with accuracy analysis\n",
    "âœ… Discussed accuracy differences and possible causes\n",
    "\n",
    "### ðŸ“ All Results Saved to Google Drive:\n",
    "Check your Google Drive folder: `MyDrive/african_wildlife_project/`\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Next Steps (Optional):\n",
    "1. Try unfreezing some base model layers and fine-tuning further\n",
    "2. Implement data augmentation for better generalization\n",
    "3. Experiment with ResNet50 for comparison\n",
    "4. Increase dataset size for improved accuracy\n",
    "\n",
    "**Happy Learning! ðŸŽ“**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
